{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Dection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import openvino as ov\n",
    "from typing import Any\n",
    "import json\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from models.rtmpose.deploy_infer import (\n",
    "    Compose,\n",
    "    GetBBoxCenterScale,\n",
    "    LoadImage,\n",
    "    PackPoseInputs,\n",
    "    PoseDataPreprocessor,\n",
    "    TopdownAffine,\n",
    "    prepare_data,\n",
    "    restore_keypoints,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requirements_demo.txt` contains packages and their verision that are be required to run this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_MODELS = [\n",
    "    \"rtm_body8_26keypoints_pose-m_256x192\",\n",
    "    \"rtm_coco_pose-l\",\n",
    "    \"rtm_body8_26keypoints_pose-m_384x288\",\n",
    "    \"rtm_coco_pose-m\",\n",
    "    \"rtm_body8_pose-s\",\n",
    "    \"rtm_body8_pose-m\",\n",
    "]\n",
    "\n",
    "MODEL_NM = \"rtm_body8_26keypoints_pose-m_384x288\"\n",
    "\n",
    "assert MODEL_NM in DEPLOYED_MODELS, f\"Model {MODEL_NM} not found in DEPLOYED_MODELS\"\n",
    "\n",
    "if \"384x288\" in MODEL_NM:\n",
    "    INPUT_SIZE = 288, 384\n",
    "    OUTPUT_SIZE = 576, 768\n",
    "else:\n",
    "    INPUT_SIZE = 192, 256\n",
    "    OUTPUT_SIZE = 384, 512\n",
    "\n",
    "if \"26keypoints\" in MODEL_NM:\n",
    "    N_KEYPOINTS = 26\n",
    "else:\n",
    "    N_KEYPOINTS = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "model = f\"./out/{MODEL_NM}/{MODEL_NM}.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.4 ms ± 601 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "compiled_model = core.compile_model(\n",
    "    model=model,\n",
    "    device_name=\"CPU\",  # , config={\"DYN_BATCH_ENABLED\": \"YES\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(\n",
    "    model=model,\n",
    "    device_name=\"CPU\",  # , config={\"DYN_BATCH_ENABLED\": \"YES\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CompiledModel:\n",
       "inputs[\n",
       "<ConstOutput: names[image] shape[?,3,384,288] type: f32>\n",
       "]\n",
       "outputs[\n",
       "<ConstOutput: names[707] shape[?,26,576] type: f32>,\n",
       "<ConstOutput: names[709] shape[?,26,768] type: f32>\n",
       "]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model have dynamic batch size to allow running them once per image (that could have multiple people in it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing from oryginal repo depended on multiple packages (mmpose, mmcv, mmengine) this introduces many restrictions (e.g. the appropriate version of mmcv will not work with python>=3.10, torch>=2.0).\n",
    "\n",
    "To overcome this all necessary transformation have been extracted to `models.rtmpose.deploy_mmpose_replacement.py` (leaving only dependency on cv2, numpy, torch but if this packages won't introduce breaking change code should be fine and it is working with lastest version (e.g. we can use python3.11 and torch 2.3.1))\n",
    "\n",
    "Those depending on use case you might want to change preprocessing steps (probably LoadImage and PoseDataPreprocessor might need to be changed depending on what is the input to the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline works on whole image and bboxes (preprocessing steps have some parameters to steer things like padding, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Compose(\n",
    "    [\n",
    "        LoadImage(),\n",
    "        GetBBoxCenterScale(),\n",
    "        TopdownAffine(input_size=INPUT_SIZE),\n",
    "        PackPoseInputs(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessor = PoseDataPreprocessor(\n",
    "    mean=[123.675, 116.28, 103.53],\n",
    "    std=[58.395, 57.12, 57.375],\n",
    "    bgr_to_rgb=True,  # if isinstance(img, np.ndarray) then check if bgr_to_rgb is required\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(img: str | np.ndarray, bboxes: np.ndarray = None):\n",
    "    if bboxes is None:\n",
    "        h, w, _ = img.shape\n",
    "        bboxes = np.array([[0, 0, w, h]], dtype=np.float32)\n",
    "\n",
    "    data_list, preprocessed_image = prepare_data(\n",
    "        img, bboxes, pipeline, data_preprocessor\n",
    "    )\n",
    "\n",
    "    infer_request = compiled_model.create_infer_request()\n",
    "\n",
    "    input_tensor = ov.Tensor(array=preprocessed_image.numpy(), shared_memory=True)\n",
    "    infer_request.set_input_tensor(input_tensor)\n",
    "\n",
    "    infer_request.set_output_tensor(\n",
    "        0,\n",
    "        ov.Tensor(\n",
    "            np.zeros((bboxes.shape[0], N_KEYPOINTS, OUTPUT_SIZE[0]), dtype=np.float32)\n",
    "        ),\n",
    "    )\n",
    "    infer_request.set_output_tensor(\n",
    "        1,\n",
    "        ov.Tensor(\n",
    "            np.zeros((bboxes.shape[0], N_KEYPOINTS, OUTPUT_SIZE[1]), dtype=np.float32)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    infer_request.start_async()\n",
    "    infer_request.wait()\n",
    "\n",
    "    simcc_x = infer_request.get_output_tensor(0).data\n",
    "    simcc_y = infer_request.get_output_tensor(1).data\n",
    "\n",
    "    openvino_pred, openvino_scores = restore_keypoints(simcc_x, simcc_y, data_list)\n",
    "\n",
    "    return {\n",
    "        \"keypoints\": openvino_pred,\n",
    "        \"keypoint_scores\": openvino_scores,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(file_path: str) -> Any:\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\n",
    "def load_img(filename: str) -> np.ndarray:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        value = f.read()\n",
    "    img_np = np.frombuffer(value, np.uint8)\n",
    "    flag = cv2.IMREAD_COLOR\n",
    "    img = cv2.imdecode(img_np, flag)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object_1': {'ymin': 436, 'ymax': 1068, 'xmin': 1004, 'xmax': 1225},\n",
       " 'object_2': {'ymin': 443, 'ymax': 1080, 'xmin': 536, 'xmax': 842},\n",
       " 'object_3': {'ymin': 260, 'ymax': 542, 'xmin': 302, 'xmax': 390},\n",
       " 'object_4': {'ymin': 142, 'ymax': 362, 'xmin': 905, 'xmax': 970}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_json(\"wycinki/cam1_1/objects_pos.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes_to_xywh(bboxes: dict[str, dict[str, int]]) -> np.ndarray:\n",
    "    return np.array(\n",
    "        [\n",
    "            [\n",
    "                bbox[\"xmin\"],\n",
    "                bbox[\"ymin\"],\n",
    "                bbox[\"xmax\"],\n",
    "                bbox[\"ymax\"],\n",
    "            ]\n",
    "            for bbox in bboxes.values()\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_root = \"./wycinki/cam1_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding boxes needs to be in format [[x, y, w, h], ...] there is example how to prepare it from `objects_pos.json` file that was delivered to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1004,  436, 1225, 1068],\n",
       "       [ 536,  443,  842, 1080],\n",
       "       [ 302,  260,  390,  542],\n",
       "       [ 905,  142,  970,  362]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes = bboxes_to_xywh(load_json(os.path.join(img_root, \"objects_pos.json\")))\n",
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(os.path.join(img_root, \"image.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specified pipeline works for the images as numpy arrays in shape (w, h, d) (bgr_to_rgb=True in PoseDataPreprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1080, 1920, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference function returns dict with keypoints and keypoint_scores.\n",
    "\n",
    "- keypoints: contains numpy array with shape (n, k, 2)\n",
    "- keypoint_scores: contains numpy array with shape (n, k)\n",
    "\n",
    "where\n",
    "\n",
    "* n - number of bboxes,\n",
    "* k - number of keypoints (depending on model),\n",
    "* 2 - (x, y) coordinates \n",
    "\n",
    "The default threshold for keypoint to be assigned as recognized is 0.3 (used for visualization, change it if you want to display keypoint about which model is more sure of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.4 ms ± 1.59 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = inference(img, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = inference(img, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['keypoints', 'keypoint_scores'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 26, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"keypoints\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"keypoint_scores\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File `models/rtmpose/utils.py` contains `to_key_points` function that transforms keypoints and defines config (skeleton) that is later used by `draw_pose.get_pose` function to draw visualization of each person pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.rtmpose.utils as rtm_utils\n",
    "import draw_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses, config = rtm_utils.to_key_points(results, threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path: str) -> np.ndarray:\n",
    "    return cv2.imread(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = load_image(os.path.join(img_root, \"image.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for pose in poses:\n",
    "    poses, config = rtm_utils.to_key_points(results, threshold=0.999)\n",
    "    input_img = draw_pose.get_pose(\n",
    "        input_img, key_points=pose, edges=config, line_width=3\n",
    "    )\n",
    "# draw_pose.draw_pose()\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(input_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of cells above is not present due to uncertainty of whether the image can be publicly shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
